{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y libopenmpi-dev\n",
        "!pip install mpi4py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfhmPywkg-ZZ",
        "outputId": "f5ca391b-9df6-4b7c-b649-9b0726394b08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libopenmpi-dev is already the newest version (4.1.2-2ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "Requirement already satisfied: mpi4py in /usr/local/lib/python3.11/dist-packages (4.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/mpi4py_examples.py\n",
        "\"\"\"\n",
        "MPI4PY: Distributed-Memory Parallelization Examples\n",
        "--------------------------------------------------\n",
        "This file contains multiple examples demonstrating distributed-memory parallelization\n",
        "using MPI4PY, inspired by the examples in Universidad Politecnica de Yucatan's report.\n",
        "\n",
        "To run any example:\n",
        "    mpiexec -n <number_of_processes> python mpi4py_examples.py --example <example_number>\n",
        "\n",
        "Examples:\n",
        "1. Hello World - Basic MPI initialization and rank identification\n",
        "2. Array Math - Distributed array computation with workload partitioning\n",
        "3. Monte Carlo Simulation - Lennard-Jones particle simulation\n",
        "4. Image Spectrogram - Parallel FFT processing of images\n",
        "5. Matrix-Vector Product - Distributed matrix-vector multiplication\n",
        "\n",
        "Author: Krishna\n",
        "Date: April 8, 2025\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "import math\n",
        "from mpi4py import MPI\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter\n",
        "\n",
        "\n",
        "def parse_arguments():\n",
        "    \"\"\"Parse command line arguments to select which example to run.\"\"\"\n",
        "    parser = argparse.ArgumentParser(description='MPI4PY Examples')\n",
        "    parser.add_argument('--example', type=int, default=1, choices=[1, 2, 3, 4, 5],\n",
        "                        help='Example number to run (1-5)')\n",
        "    parser.add_argument('--size', type=int, default=10000000,\n",
        "                        help='Problem size for relevant examples')\n",
        "    parser.add_argument('--iterations', type=int, default=10,\n",
        "                        help='Number of iterations for relevant examples')\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "# Example 1: Hello World with MPI\n",
        "# ======================================================================\n",
        "def example1_hello_world():\n",
        "    \"\"\"\n",
        "    A simple example showing MPI initialization and process rank identification.\n",
        "    Each process prints its rank and the total number of processes.\n",
        "    \"\"\"\n",
        "    comm = MPI.COMM_WORLD\n",
        "    size = comm.Get_size()\n",
        "    rank = comm.Get_rank()\n",
        "\n",
        "    # Each process prints its rank\n",
        "    print(f\"Hello World from rank {rank} of {size}\")\n",
        "\n",
        "    # Barrier to ensure all processes complete before proceeding\n",
        "    comm.Barrier()\n",
        "\n",
        "    # For demonstration, have rank 0 print a summary\n",
        "    if rank == 0:\n",
        "        print(f\"\\nCompleted Example 1: Hello World with {size} processes\")\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "# Example 2: Array Math with Timing and Workload Partitioning\n",
        "# ======================================================================\n",
        "def example2_array_math(N=10000000):\n",
        "    \"\"\"\n",
        "    Demonstrates distributed array computation with workload partitioning.\n",
        "    Each process handles a portion of two large arrays, computes element-wise\n",
        "    addition, and collectively calculates the average.\n",
        "\n",
        "    Args:\n",
        "        N: Size of arrays to process\n",
        "    \"\"\"\n",
        "    comm = MPI.COMM_WORLD\n",
        "    size = comm.Get_size()\n",
        "    rank = comm.Get_rank()\n",
        "\n",
        "    start_time = MPI.Wtime()\n",
        "\n",
        "    # Determine workload distribution\n",
        "    workloads = [N // size for _ in range(size)]\n",
        "    # Distribute remainder to first few processes\n",
        "    for i in range(N % size):\n",
        "        workloads[i] += 1\n",
        "\n",
        "    # Calculate start and end indices for this process\n",
        "    my_start = sum(workloads[:rank])\n",
        "    my_end = my_start + workloads[rank]\n",
        "\n",
        "    if rank == 0:\n",
        "        print(f\"Processing array of size {N} with {size} processes\")\n",
        "        print(f\"Workload distribution: {workloads}\")\n",
        "\n",
        "    # Memory optimization: allocate only what this process needs\n",
        "    a = np.ones(workloads[rank], dtype=np.float64)\n",
        "    b = np.zeros(workloads[rank], dtype=np.float64)\n",
        "\n",
        "    # Initialize array b with a pattern\n",
        "    for i in range(workloads[rank]):\n",
        "        b[i] = 1.0 + (i + my_start)\n",
        "\n",
        "    # Perform element-wise addition\n",
        "    computation_start = MPI.Wtime()\n",
        "    a += b  # Vectorized operation\n",
        "    computation_time = MPI.Wtime() - computation_start\n",
        "\n",
        "    # Compute local sum and prepare for reduction\n",
        "    local_sum = np.array([np.sum(a)], dtype=np.float64)\n",
        "    total_sum = np.zeros(1, dtype=np.float64)\n",
        "\n",
        "    # Perform reduction to get total sum on rank 0\n",
        "    reduction_start = MPI.Wtime()\n",
        "    comm.Reduce([local_sum, MPI.DOUBLE], [total_sum, MPI.DOUBLE], op=MPI.SUM, root=0)\n",
        "    reduction_time = MPI.Wtime() - reduction_start\n",
        "\n",
        "    # Calculate and print results on rank 0\n",
        "    if rank == 0:\n",
        "        average = total_sum[0] / N\n",
        "        end_time = MPI.Wtime()\n",
        "        total_time = end_time - start_time\n",
        "\n",
        "        print(f\"\\nResults:\")\n",
        "        print(f\"Average value: {average}\")\n",
        "        print(f\"Expected average: {(N + 3) / 2}\")  # Mathematical verification\n",
        "        print(f\"\\nPerformance:\")\n",
        "        print(f\"Total time: {total_time:.6f} seconds\")\n",
        "        print(f\"Computation time: {computation_time:.6f} seconds\")\n",
        "        print(f\"Reduction time: {reduction_time:.6f} seconds\")\n",
        "        print(f\"Completed Example 2: Array Math with {size} processes\")\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "# Example 3: Monte Carlo Simulation of Lennard-Jones Particles\n",
        "# ======================================================================\n",
        "def lennard_jones_potential(r_squared):\n",
        "    \"\"\"\n",
        "    Calculate Lennard-Jones potential energy between two particles.\n",
        "\n",
        "    Args:\n",
        "        r_squared: Square of the distance between particles\n",
        "\n",
        "    Returns:\n",
        "        Potential energy value\n",
        "    \"\"\"\n",
        "    if r_squared == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Calculate 1/r^6 and 1/r^12 terms\n",
        "    r6_inv = 1.0 / (r_squared**3)\n",
        "    r12_inv = r6_inv * r6_inv\n",
        "\n",
        "    # LJ potential: 4*epsilon*[(sigma/r)^12 - (sigma/r)^6]\n",
        "    # With epsilon=1 and sigma=1 for simplicity\n",
        "    return 4.0 * (r12_inv - r6_inv)\n",
        "\n",
        "\n",
        "def minimum_image_distance(pos_i, pos_j, box_length):\n",
        "    \"\"\"\n",
        "    Calculate minimum image distance between two particles\n",
        "    under periodic boundary conditions.\n",
        "\n",
        "    Args:\n",
        "        pos_i: Position of first particle\n",
        "        pos_j: Position of second particle\n",
        "        box_length: Length of the cubic simulation box\n",
        "\n",
        "    Returns:\n",
        "        Square of the minimum image distance\n",
        "    \"\"\"\n",
        "    diff = pos_i - pos_j\n",
        "\n",
        "    # Apply periodic boundary conditions\n",
        "    diff = diff - box_length * np.round(diff / box_length)\n",
        "\n",
        "    # Return squared distance\n",
        "    return np.sum(diff**2)\n",
        "\n",
        "\n",
        "def get_particle_energy(coordinates, box_length, i_particle, cutoff2, comm):\n",
        "    \"\"\"\n",
        "    Calculate total interaction energy for a single particle with all others,\n",
        "    distributed across MPI processes.\n",
        "\n",
        "    Args:\n",
        "        coordinates: Array of particle positions\n",
        "        box_length: Length of the cubic simulation box\n",
        "        i_particle: Index of the particle to calculate energy for\n",
        "        cutoff2: Square of the cutoff distance for interactions\n",
        "        comm: MPI communicator\n",
        "\n",
        "    Returns:\n",
        "        Total energy for the particle\n",
        "    \"\"\"\n",
        "    rank = comm.Get_rank()\n",
        "    size = comm.Get_size()\n",
        "\n",
        "    # Each process calculates a subset of interactions\n",
        "    e_total = 0.0\n",
        "    i_position = coordinates[i_particle]\n",
        "    particle_count = len(coordinates)\n",
        "\n",
        "    # Distributed loop: each process handles a subset of particles\n",
        "    for j_particle in range(rank, particle_count, size):\n",
        "        if i_particle != j_particle:\n",
        "            j_position = coordinates[j_particle]\n",
        "            rij2 = minimum_image_distance(i_position, j_position, box_length)\n",
        "\n",
        "            # Apply cutoff for efficiency\n",
        "            if rij2 < cutoff2:\n",
        "                e_total += lennard_jones_potential(rij2)\n",
        "\n",
        "    # Combine partial results using Allreduce\n",
        "    e_single = np.array([e_total], dtype=np.float64)\n",
        "    e_summed = np.zeros(1, dtype=np.float64)\n",
        "    comm.Allreduce([e_single, MPI.DOUBLE], [e_summed, MPI.DOUBLE], op=MPI.SUM)\n",
        "\n",
        "    return e_summed[0]\n",
        "\n",
        "\n",
        "def example3_monte_carlo(iterations=1000, n_particles=100):\n",
        "    \"\"\"\n",
        "    Monte Carlo simulation of Lennard-Jones particles in a periodic box.\n",
        "    Each process helps compute energies in parallel.\n",
        "\n",
        "    Args:\n",
        "        iterations: Number of Monte Carlo steps\n",
        "        n_particles: Number of particles in the simulation\n",
        "    \"\"\"\n",
        "    comm = MPI.COMM_WORLD\n",
        "    size = comm.Get_size()\n",
        "    rank = comm.Get_rank()\n",
        "\n",
        "    # Simulation parameters\n",
        "    box_length = 10.0\n",
        "    cutoff = 3.0\n",
        "    cutoff2 = cutoff * cutoff\n",
        "    temperature = 1.0\n",
        "    max_displacement = 0.1\n",
        "\n",
        "    # Initialize random number generator with same seed on all processes\n",
        "    seed = 42\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Initialize particle coordinates randomly - same on all processes\n",
        "    if rank == 0:\n",
        "        coordinates = np.random.uniform(0, box_length, (n_particles, 3))\n",
        "    else:\n",
        "        coordinates = np.zeros((n_particles, 3))\n",
        "\n",
        "    # Broadcast initial coordinates to all processes\n",
        "    comm.Bcast([coordinates, MPI.DOUBLE], root=0)\n",
        "\n",
        "    # Calculate initial total energy\n",
        "    if rank == 0:\n",
        "        print(f\"Starting Monte Carlo simulation with {n_particles} particles\")\n",
        "        print(f\"Box length: {box_length}, Temperature: {temperature}\")\n",
        "        print(f\"Running on {size} processes for {iterations} iterations\")\n",
        "\n",
        "    # Calculate initial system energy\n",
        "    total_energy = 0.0\n",
        "    for i in range(n_particles):\n",
        "        energy_i = get_particle_energy(coordinates, box_length, i, cutoff2, comm)\n",
        "        total_energy += energy_i\n",
        "\n",
        "    # Divide by 2 to avoid double counting pair interactions\n",
        "    total_energy /= 2.0\n",
        "\n",
        "    # Set up timing\n",
        "    start_time = MPI.Wtime()\n",
        "    energy_time = 0.0\n",
        "    decision_time = 0.0\n",
        "\n",
        "    # Main Monte Carlo loop\n",
        "    accepted_moves = 0\n",
        "\n",
        "    for step in range(1, iterations + 1):\n",
        "        # Each process needs the same random particle and displacement\n",
        "        if rank == 0:\n",
        "            # Select random particle\n",
        "            i_particle = random.randint(0, n_particles - 1)\n",
        "            # Generate random displacement\n",
        "            displacement = np.random.uniform(-max_displacement, max_displacement, 3)\n",
        "            random_data = np.array([i_particle, displacement[0], displacement[1], displacement[2]])\n",
        "        else:\n",
        "            random_data = np.zeros(4)\n",
        "\n",
        "        # Broadcast random choice to all processes\n",
        "        comm.Bcast([random_data, MPI.DOUBLE], root=0)\n",
        "        i_particle = int(random_data[0])\n",
        "        displacement = random_data[1:4]\n",
        "\n",
        "        # Calculate energy before move\n",
        "        energy_start = MPI.Wtime()\n",
        "        old_energy = get_particle_energy(coordinates, box_length, i_particle, cutoff2, comm)\n",
        "\n",
        "        # Make temporary move\n",
        "        old_position = coordinates[i_particle].copy()\n",
        "        coordinates[i_particle] += displacement\n",
        "\n",
        "        # Apply periodic boundary conditions\n",
        "        coordinates[i_particle] = coordinates[i_particle] % box_length\n",
        "\n",
        "        # Calculate energy after move\n",
        "        new_energy = get_particle_energy(coordinates, box_length, i_particle, cutoff2, comm)\n",
        "        energy_time += MPI.Wtime() - energy_start\n",
        "\n",
        "        # Accept or reject move using Metropolis criterion\n",
        "        decision_start = MPI.Wtime()\n",
        "        delta_energy = new_energy - old_energy\n",
        "\n",
        "        # Metropolis acceptance criterion\n",
        "        if delta_energy <= 0 or random.random() < math.exp(-delta_energy / temperature):\n",
        "            # Accept move\n",
        "            accepted_moves += 1\n",
        "            total_energy += delta_energy\n",
        "        else:\n",
        "            # Reject move - revert to old position\n",
        "            coordinates[i_particle] = old_position\n",
        "\n",
        "        decision_time += MPI.Wtime() - decision_start\n",
        "\n",
        "        # Print progress and current energy\n",
        "        if rank == 0 and step % 100 == 0:\n",
        "            print(f\"Step {step}: Energy = {total_energy:.6f}, \"\n",
        "                  f\"Acceptance rate = {accepted_moves / step:.2f}\")\n",
        "\n",
        "    end_time = MPI.Wtime()\n",
        "    total_time = end_time - start_time\n",
        "\n",
        "    # Print final results\n",
        "    if rank == 0:\n",
        "        print(f\"\\nMonte Carlo simulation completed:\")\n",
        "        print(f\"Final energy: {total_energy:.6f}\")\n",
        "        print(f\"Acceptance rate: {accepted_moves / iterations:.4f}\")\n",
        "        print(f\"\\nPerformance:\")\n",
        "        print(f\"Total time: {total_time:.6f} seconds\")\n",
        "        print(f\"Energy calculation time: {energy_time:.6f} seconds ({energy_time/total_time*100:.1f}%)\")\n",
        "        print(f\"Decision time: {decision_time:.6f} seconds ({decision_time/total_time*100:.1f}%)\")\n",
        "        print(f\"Completed Example 3: Monte Carlo with {size} processes\")\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "# Example 4: Image Spectrogram - Parallel FFT Processing\n",
        "# ======================================================================\n",
        "def generate_sample_images(n_images=100, image_size=64):\n",
        "    \"\"\"\n",
        "    Generate synthetic images for the spectrogram example.\n",
        "\n",
        "    Args:\n",
        "        n_images: Number of images to generate\n",
        "        image_size: Size of each square image\n",
        "\n",
        "    Returns:\n",
        "        Array of images with shape (n_images, image_size, image_size)\n",
        "    \"\"\"\n",
        "    # Create array to hold all images\n",
        "    images = np.zeros((n_images, image_size, image_size))\n",
        "\n",
        "    # Generate images with different patterns\n",
        "    for i in range(n_images):\n",
        "        # Pattern type based on image index\n",
        "        pattern_type = i % 4\n",
        "\n",
        "        if pattern_type == 0:\n",
        "            # Sinusoidal pattern\n",
        "            x = np.linspace(0, 6*np.pi, image_size)\n",
        "            y = np.linspace(0, 6*np.pi, image_size)\n",
        "            X, Y = np.meshgrid(x, y)\n",
        "            freq = 1.0 + (i % 10) / 10.0\n",
        "            images[i] = np.sin(X * freq) * np.cos(Y * freq)\n",
        "\n",
        "        elif pattern_type == 1:\n",
        "            # Circular pattern\n",
        "            x = np.linspace(-1, 1, image_size)\n",
        "            y = np.linspace(-1, 1, image_size)\n",
        "            X, Y = np.meshgrid(x, y)\n",
        "            R = np.sqrt(X**2 + Y**2)\n",
        "            radius = 0.3 + (i % 8) / 20.0\n",
        "            images[i] = (R < radius).astype(float)\n",
        "\n",
        "        elif pattern_type == 2:\n",
        "            # Diagonal stripes\n",
        "            x = np.linspace(0, 1, image_size)\n",
        "            y = np.linspace(0, 1, image_size)\n",
        "            X, Y = np.meshgrid(x, y)\n",
        "            freq = 5 + (i % 10)\n",
        "            images[i] = np.sin(freq * (X + Y))\n",
        "\n",
        "        else:\n",
        "            # Random noise with varying frequency content\n",
        "            noise_scale = 1.0 + (i % 6) / 2.0\n",
        "            images[i] = np.random.randn(image_size, image_size)\n",
        "            # Smooth with Gaussian filter of varying width\n",
        "            images[i] = gaussian_filter(images[i], sigma=noise_scale)\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "def example4_image_spectrogram(n_images=100, image_size=64):\n",
        "    \"\"\"\n",
        "    Compute and visualize the average 2D spectrogram of multiple images\n",
        "    using parallel FFT processing.\n",
        "\n",
        "    Args:\n",
        "        n_images: Number of images to process\n",
        "        image_size: Size of each square image\n",
        "    \"\"\"\n",
        "    comm = MPI.COMM_WORLD\n",
        "    size = comm.Get_size()\n",
        "    rank = comm.Get_rank()\n",
        "\n",
        "    # Setup timing\n",
        "    start_time = MPI.Wtime()\n",
        "\n",
        "    if rank == 0:\n",
        "        print(f\"Processing {n_images} images of size {image_size}x{image_size}\")\n",
        "        print(f\"Using {size} MPI processes for parallel FFT computation\")\n",
        "\n",
        "        # Generate or load sample images\n",
        "        # In real application, these would be loaded from a file\n",
        "        images = generate_sample_images(n_images, image_size)\n",
        "    else:\n",
        "        # Other ranks don't need the full images yet\n",
        "        images = None\n",
        "\n",
        "    # Broadcast number of images to all processes\n",
        "    if rank == 0:\n",
        "        n_images_data = np.array([n_images], dtype=np.int32)\n",
        "    else:\n",
        "        n_images_data = np.zeros(1, dtype=np.int32)\n",
        "\n",
        "    comm.Bcast([n_images_data, MPI.INT], root=0)\n",
        "    n_images = n_images_data[0]\n",
        "\n",
        "    # Initialize local spectrogram accumulator\n",
        "    local_spectrogram = np.zeros((image_size, image_size), dtype=np.float64)\n",
        "\n",
        "    # Determine which images this rank will process\n",
        "    my_images = []\n",
        "    for i in range(n_images):\n",
        "        if i % size == rank:\n",
        "            my_images.append(i)\n",
        "\n",
        "    # Process assigned images\n",
        "    for img_idx in my_images:\n",
        "        # Receive image from rank 0 if needed\n",
        "        if rank == 0:\n",
        "            img = images[img_idx]\n",
        "        else:\n",
        "            img = np.zeros((image_size, image_size), dtype=np.float64)\n",
        "            comm.Recv([img, MPI.DOUBLE], source=0, tag=img_idx)\n",
        "\n",
        "        # Compute 2D FFT\n",
        "        img_fft = np.fft.fft2(img)\n",
        "\n",
        "        # Shift zero frequency to center\n",
        "        img_fft_shifted = np.fft.fftshift(img_fft)\n",
        "\n",
        "        # Compute magnitude and add to local spectrogram\n",
        "        local_spectrogram += np.abs(img_fft_shifted)\n",
        "\n",
        "    # Non-rank 0 processes need to send their images\n",
        "    if rank == 0:\n",
        "        for i in range(n_images):\n",
        "            if i % size != 0:  # Not processed by rank 0\n",
        "                target_rank = i % size\n",
        "                comm.Send([images[i], MPI.DOUBLE], dest=target_rank, tag=i)\n",
        "\n",
        "    # Reduce local spectrograms to create global spectrogram on rank 0\n",
        "    if rank == 0:\n",
        "        global_spectrogram = np.zeros_like(local_spectrogram)\n",
        "    else:\n",
        "        global_spectrogram = None\n",
        "\n",
        "    comm.Reduce([local_spectrogram, MPI.DOUBLE],\n",
        "                [global_spectrogram, MPI.DOUBLE],\n",
        "                op=MPI.SUM, root=0)\n",
        "\n",
        "    # Process and display results on rank 0\n",
        "    if rank == 0:\n",
        "        # Normalize by number of images\n",
        "        global_spectrogram /= n_images\n",
        "\n",
        "        # Convert to log scale for better visualization\n",
        "        log_spectrogram = np.log(1 + global_spectrogram)\n",
        "\n",
        "        # Calculate total time\n",
        "        total_time = MPI.Wtime() - start_time\n",
        "\n",
        "        print(f\"\\nSpectrogram computation completed:\")\n",
        "        print(f\"Processed {n_images} images in {total_time:.6f} seconds\")\n",
        "        print(f\"Processing rate: {n_images / total_time:.2f} images per second\")\n",
        "\n",
        "        # Display the spectrogram\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.imshow(log_spectrogram, cmap='viridis')\n",
        "        plt.colorbar(label='Log magnitude')\n",
        "        plt.title('Average Image Spectrogram (Log Scale)')\n",
        "        plt.xlabel('Frequency (x)')\n",
        "        plt.ylabel('Frequency (y)')\n",
        "\n",
        "        # Save the figure instead of displaying for non-GUI environments\n",
        "        plt.savefig('/content/spectrogram.png')\n",
        "        print(\"Spectrogram saved as '/content/spectrogram.png'\")\n",
        "\n",
        "        print(f\"Completed Example 4: Image Spectrogram with {size} processes\")\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "# Example 5: Matrix-Vector Product - Iterative Solver Simulation\n",
        "# ======================================================================\n",
        "def example5_matrix_vector_product(vector_size=10000, iterations=20):\n",
        "    \"\"\"\n",
        "    Perform distributed matrix-vector multiplication iteratively.\n",
        "    Each process computes a slice of the matrix-vector product, and\n",
        "    all processes share the updated vector after each iteration.\n",
        "\n",
        "    Args:\n",
        "        vector_size: Size of the vector\n",
        "        iterations: Number of iterations to perform\n",
        "    \"\"\"\n",
        "    comm = MPI.COMM_WORLD\n",
        "    size = comm.Get_size()\n",
        "    rank = comm.Get_rank()\n",
        "\n",
        "    # Ensure vector size is divisible by number of processes\n",
        "    if vector_size % size != 0:\n",
        "        if rank == 0:\n",
        "            print(f\"Warning: Vector size {vector_size} not divisible by {size} processes\")\n",
        "            print(f\"Adjusting vector size to {vector_size + (size - vector_size % size)}\")\n",
        "        vector_size += (size - vector_size % size)\n",
        "\n",
        "    # Calculate local vector size and offset\n",
        "    local_size = vector_size // size\n",
        "    offset = rank * local_size\n",
        "\n",
        "    # Initialize vector [0, 0, 0, ..., 0] with first element as 1.0\n",
        "    # This is a simple test case where value should shift by one position each iteration\n",
        "    if rank == 0:\n",
        "        v = np.zeros(vector_size, dtype=np.float64)\n",
        "        v[0] = 1.0\n",
        "        print(f\"Starting matrix-vector product with vector size {vector_size}\")\n",
        "        print(f\"Running {iterations} iterations on {size} processes\")\n",
        "    else:\n",
        "        v = np.zeros(vector_size, dtype=np.float64)\n",
        "\n",
        "    # Broadcast initial vector to all processes\n",
        "    comm.Bcast([v, MPI.DOUBLE], root=0)\n",
        "\n",
        "    # Allocate local vector chunk\n",
        "    local_v = np.zeros(local_size, dtype=np.float64)\n",
        "    new_local_v = np.zeros(local_size, dtype=np.float64)\n",
        "\n",
        "    # Start timer\n",
        "    start_time = MPI.Wtime()\n",
        "\n",
        "    # Define the matrix structure\n",
        "    # For this example, we use a \"shift\" matrix where each row i has 1.0 at position i-1\n",
        "    # This matrix shifts the vector by one element per iteration\n",
        "\n",
        "    # Main iteration loop\n",
        "    for iter in range(iterations):\n",
        "        # Copy global vector to local chunk for easier access\n",
        "        local_v = v[offset:offset+local_size].copy()\n",
        "\n",
        "        # Matrix-vector product for local slice\n",
        "        for i in range(local_size):\n",
        "            # Global index of this row\n",
        "            global_i = offset + i\n",
        "\n",
        "            # For the \"shift\" matrix, set v[i] = v[i-1]\n",
        "            if global_i > 0:\n",
        "                new_local_v[i] = v[global_i-1]\n",
        "            else:\n",
        "                # First element becomes zero (or could wrap around)\n",
        "                new_local_v[i] = 0.0\n",
        "\n",
        "        # Gather all chunks of the new vector\n",
        "        # First, all processes send their local results to all others\n",
        "        comm.Allgather([new_local_v, MPI.DOUBLE], [v, MPI.DOUBLE])\n",
        "\n",
        "        # Optionally print state for debugging (only for small vectors)\n",
        "        if vector_size <= 20 and rank == 0:\n",
        "            print(f\"Iteration {iter+1}: {v[:20]}\")\n",
        "\n",
        "    # Calculate elapsed time\n",
        "    end_time = MPI.Wtime()\n",
        "    elapsed = end_time - start_time\n",
        "\n",
        "    # Print results\n",
        "    if rank == 0:\n",
        "        # For large vectors, just show beginning and end\n",
        "        if vector_size > 20:\n",
        "            print(\"\\nFinal vector (first 10 elements):\")\n",
        "            print(v[:10])\n",
        "        else:\n",
        "            print(\"\\nFinal vector:\")\n",
        "            print(v)\n",
        "\n",
        "        print(f\"\\nMatrix-vector product completed:\")\n",
        "        print(f\"{iterations} iterations of size {vector_size} in {elapsed:.2f} seconds\")\n",
        "        print(f\"Performance: {iterations / elapsed:.2f} iterations per second\")\n",
        "        print(f\"Completed Example 5: Matrix-Vector Product with {size} processes\")\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "# Main function to run selected example\n",
        "# ======================================================================\n",
        "def main():\n",
        "    \"\"\"Main function to run selected example based on command line arguments.\"\"\"\n",
        "    try:\n",
        "        args = parse_arguments()\n",
        "\n",
        "        # Run selected example\n",
        "        if args.example == 1:\n",
        "            example1_hello_world()\n",
        "        elif args.example == 2:\n",
        "            example2_array_math(args.size)\n",
        "        elif args.example == 3:\n",
        "            example3_monte_carlo(args.iterations * 100, 100)\n",
        "        elif args.example == 4:\n",
        "            example4_image_spectrogram(100, 64)\n",
        "        elif args.example == 5:\n",
        "            example5_matrix_vector_product(args.size // 1000, args.iterations)\n",
        "        else:\n",
        "            print(f\"Invalid example number: {args.example}\")\n",
        "    except Exception as e:\n",
        "        # Si hay un error al analizar los argumentos, ejecutar ejemplo 1 por defecto\n",
        "        print(f\"Error parsing arguments: {e}\")\n",
        "        print(\"Running default example (Hello World)\")\n",
        "        example1_hello_world()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZCOQFudgWm4",
        "outputId": "111e9436-0df7-4411-d116-579d854a7454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/mpi4py_examples.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/test_example1.py\n",
        "from mpi4py_examples import example1_hello_world\n",
        "\n",
        "# Ejecutar el ejemplo Hello World\n",
        "example1_hello_world()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bL05oKmDwAF9",
        "outputId": "83f60e70-beaf-44ec-db58-02d285da31d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/test_example1.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/test_example2.py\n",
        "from mpi4py_examples import example2_array_math\n",
        "\n",
        "# Ejecutar con un tamaño reducido para que termine más rápido\n",
        "example2_array_math(1000000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGp20ltNwACm",
        "outputId": "3a0adfed-eb54-485b-9f5a-735d87868ea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/test_example2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/test_example3.py\n",
        "from mpi4py_examples import example3_monte_carlo\n",
        "\n",
        "# Ejecutar con valores reducidos para que termine más rápido\n",
        "example3_monte_carlo(iterations=200, n_particles=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfmC8ry5wAAH",
        "outputId": "765fc5b3-5d9c-4d89-b243-79407f928387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/test_example3.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/test_example4.py\n",
        "from mpi4py_examples import example4_image_spectrogram\n",
        "\n",
        "# Ejecutar con valores reducidos\n",
        "example4_image_spectrogram(n_images=20, image_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "micdRSc_v_wQ",
        "outputId": "d73aa62f-2774-47c4-8633-9e2d6bb57a63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/test_example4.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/test_example5.py\n",
        "from mpi4py_examples import example5_matrix_vector_product\n",
        "\n",
        "# Ejecutar con valores reducidos\n",
        "example5_matrix_vector_product(vector_size=1000, iterations=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JF7t1eWwIOF",
        "outputId": "d83d2b23-3375-4102-de7d-71c0c5be70d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/test_example5.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/test_example1.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0HhyE20wIKb",
        "outputId": "ea6e4a3f-df24-40fa-d840-57119af735bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World from rank 0 of 1\n",
            "\n",
            "Completed Example 1: Hello World with 1 processes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/test_example2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlkF0R-owIH0",
        "outputId": "1dcd62d1-7cc5-4566-992d-a54be8c75038"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing array of size 1000000 with 1 processes\n",
            "Workload distribution: [1000000]\n",
            "\n",
            "Results:\n",
            "Average value: 500001.5\n",
            "Expected average: 500001.5\n",
            "\n",
            "Performance:\n",
            "Total time: 0.183877 seconds\n",
            "Computation time: 0.001552 seconds\n",
            "Reduction time: 0.000135 seconds\n",
            "Completed Example 2: Array Math with 1 processes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/test_example3.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atlvIhIfwN1h",
        "outputId": "4e3c45d5-c8dd-44f6-ab31-dec94a2f584f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Monte Carlo simulation with 50 particles\n",
            "Box length: 10.0, Temperature: 1.0\n",
            "Running on 1 processes for 200 iterations\n",
            "Step 100: Energy = 246.875750, Acceptance rate = 0.94\n",
            "Step 200: Energy = 18.194883, Acceptance rate = 0.93\n",
            "\n",
            "Monte Carlo simulation completed:\n",
            "Final energy: 18.194883\n",
            "Acceptance rate: 0.9300\n",
            "\n",
            "Performance:\n",
            "Total time: 0.213577 seconds\n",
            "Energy calculation time: 0.209847 seconds (98.3%)\n",
            "Decision time: 0.000369 seconds (0.2%)\n",
            "Completed Example 3: Monte Carlo with 1 processes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/test_example4.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULYOderFwNyy",
        "outputId": "e4d8bde9-ad25-4257-8a6c-069350298681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 20 images of size 32x32\n",
            "Using 1 MPI processes for parallel FFT computation\n",
            "\n",
            "Spectrogram computation completed:\n",
            "Processed 20 images in 0.007947 seconds\n",
            "Processing rate: 2516.77 images per second\n",
            "Spectrogram saved as '/content/spectrogram.png'\n",
            "Completed Example 4: Image Spectrogram with 1 processes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/test_example5.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_BPxMkhwNvf",
        "outputId": "e77c9180-6172-4103-ecd8-589137b6c775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting matrix-vector product with vector size 1000\n",
            "Running 10 iterations on 1 processes\n",
            "\n",
            "Final vector (first 10 elements):\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "\n",
            "Matrix-vector product completed:\n",
            "10 iterations of size 1000 in 0.00 seconds\n",
            "Performance: 2068.04 iterations per second\n",
            "Completed Example 5: Matrix-Vector Product with 1 processes\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}